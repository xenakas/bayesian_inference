{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is the Bayesian Inference controversial?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bayesians claim that the parameters are random so that their credible interval is a valid probability argument. This interpretation looks nice but the credible interval of the parameters of interest not only depends on the likelihood but also the prior, which is usually hard to obtain.\n",
    "\n",
    "2. When the likelihood and prior is complicated, the inference has to rely on the MCMC sampling, which can be really slow in most of the real-world cases.\n",
    "\n",
    "3. The biggest controversy about Bayesian inference is that you must quantify your prior knowledge about the question at hand. This makes it possible to actually influence your results, either accidentally or on purpose.\n",
    "\n",
    "This is a genuine concern, and any Bayesian analysis worth it’s salt must demonstrate that the chosen priors aren’t influencing the final results. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Bayesian Inference](http://pages.stat.wisc.edu/~larget/stat302/bayes.pdf)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prior and Posterior Distributions, Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before seeing data, the prior distribution of an unknown parameter θ is described by a\n",
    "probability density (or mass function, if discrete)  $f(\\theta)$  The Bayesian approach connects\n",
    "data and parameter through the likelihood function, $f(x \\mid \\theta)$ . The function where x is fixed as the data and the parameter $\\theta$ is what varies\n",
    "is called the likelihood. Parameter values where the likelihood is high are those that have\n",
    "a high probability of producing the observed data.   In the maximum likelihood approach to\n",
    "statistics, the best estimate of the value $\\hat{\\theta}$ that maximizes the likelihood (and log-likelihood)\n",
    "function.\n",
    "\n",
    " All Bayesian inference is based on evaluation of the posterior distribution: \n",
    " $$f(\\theta \\mid x) = \\dfrac{f(x \\mid \\theta)f(\\theta)}{f(x)}$$\n",
    " where $f(x) = \\int f(x \\mid \\theta)f(\\theta) d \\theta $ is the marginal likelihood f(x), which is the probability\n",
    "of observing the data $x$ averaged across the entire parameter space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Hierarchical Bayes and Empirical Bayes](https://www2.isye.gatech.edu/~brani/isyebayes/bank/handout8.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Bayes and Empirical Bayes are related by their goals, but quite different by the methods of how\n",
    "these goals are achieved.\n",
    "\n",
    "Both methods are concerned in specifying the distribution at prior level, hierarchical\n",
    "via Bayes inference involving additional degrees of hierarchy (hyperpriors and hyperparameters),\n",
    "while empirical Bayes is using data more directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Bayesian Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Bayesian Analysis is a convenient representation of a Bayesian model, in particular the prior\n",
    "$\\pi$, via a conditional hierarchy of so called hyper-priors $\\pi_1, \\dots, \\pi_{n+1}$,\n",
    "$$ \\pi(\\theta) = \\int \\pi_1(\\theta|\\theta_1) \\pi_2(\\theta_1|\\theta_2) \\dots  \\pi_n(\\theta_{n-1}|\\theta_n)\\pi_{n+1}(\\theta_n) d\\theta_1d\\theta_2 \\dots d\\theta_n$$ \n",
    "\n",
    "Operationally, the model: \n",
    "$$[x|\\theta] \\sim f(x|\\theta)$$ $$[\\theta|\\theta_1]  \\sim \\pi_1(\\theta|\\theta_1) $$ $$ [\\theta_{n−1}|\\theta_n]  \\sim \\pi_n(\\theta|\\theta_1)$$ $$ [\\theta_n] \\sim \\pi_{n+1}(\\theta_n)$$ \n",
    "is equivalent to the model\n",
    "$$[x|\\theta] \\sim f(x|\\theta), [\\theta] \\sim \\pi(\\theta)$$ \n",
    "as the inference on θ is concerned. \n",
    "\n",
    "Notice that in the hierarchy of data, parameters and hyperparameters,\n",
    "$X$ and $\\theta_i$ are independent, given $\\theta$: $ X \\to  \\theta \\to \\theta_1 \\to \\dots \\to \\theta_n$\n",
    "\n",
    "That means, $$[X|\\theta, \\theta_1, \\dots ] \\overset{\\mathrm{d}}{=}  [X|\\theta], [\\theta_i|\\theta, X] \\overset{\\mathrm{d}}{=} [\\theta_i\n",
    "|\\theta] $$ \n",
    "the joint distribution  which by definition is\n",
    "$$[X, \\theta, \\theta_1, \\dots, \\theta_n] = [X|\\theta, \\theta_1, \\dots, \\theta_n] [\\theta|\\theta_1, \\dots , \\theta_n] [\\theta_1|\\theta_2, \\dots, \\theta_n] \\dots [\\theta_{n−1}|\\theta_n] [\\theta_n] = [X|\\theta][\\theta|\\theta_1] [\\theta_1|\\theta_2] \\dots [\\theta_{n−1}|\\theta_n] [\\theta_n]$$\n",
    "thus, to fully specify the model, only “neighbouring” conditionals and\n",
    "the “closure” distribution $[\\theta_n]$ are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modeling requirements may lead to the hierarchy in the prior\n",
    "- Robustness and objectiveness (let the data talk about the hyperparameters)\n",
    "- Calculational issues (utilizing hidden mixtures, mixture priors, missing data, MCMC format).\n",
    "\n",
    "Sometimes it is not calculatingly feasible to carry out the analysis by reducing the sequence of hyperpriors to a single prior.\n",
    "\n",
    "Rather, Bayes rule is obtained (by using [Fubini’s theorem](http://ru.math.wikia.com/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%A2%D0%BE%D0%BD%D0%B5%D0%BB%D0%BB%D0%B8_%E2%80%94_%D0%A4%D1%83%D0%B1%D0%B8%D0%BD%D0%B8)) as repeated integral with respect to more\n",
    "convenient conditional distributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empirical Bayes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical Bayes is an approach to inference in which the observations are used to select the prior, usually\n",
    "via the marginal distribution. Once the prior is specified, the inference proceed in a standard Bayesian\n",
    "fashion. The use of data to estimate the prior in addition to subsequent use for the inference in empirical\n",
    "Bayes is criticized by subjectivists who consider the prior information exogenous to observations. The\n",
    "repeated use of data is also loaded with perils since it can underestimate modeling errors. Any data is going\n",
    "to be complacent with a model which used the same data to specify some of its features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: \n",
    "1. We find the marginal for $X_i$ in the experiment $i$\n",
    "2. Estimate $\\alpha$ and $\\beta$ in the marginal.\n",
    "3. Express $P(0 \\leq \\theta_{I+1} \\leq  0.2|X_{I+1} = 0)$ in terms of incomplete Beta functions with estimated hyperparameters $\\hat{\\alpha}, \\hat{\\beta}$\n",
    "\n",
    "Nonparametric empirical Bayes follows and uses the historic data to estimate the\n",
    "marginal distribution in nonparametric fashion. The estimated marginal distributions are then plugged in the\n",
    "formal Bayes rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ML II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to mimic the maximum likelihood estimation at the marginal level: Select a prior $\\pi$ that maximizes $m_{\\pi}(x)$,\n",
    "given the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offtop:\n",
    "\n",
    "\n",
    "- This ratio is easily numerically evaluated, see mathematica notebook. (p.3)\n",
    "- Berger (1985), Section 4.6 pages 180–195 contains an excellent account on hierarchical models with detailed proofs.\n",
    "- The result (see MATHEMATICA program jeremy.nb on the web site)\n",
    "- Marginal posterior,  marginal distribution\n",
    "- James Stein Estimator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
