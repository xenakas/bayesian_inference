{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noninformative prior distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noninformative prior distributions are intended to allow Bayesian inference for\n",
    "parameters about which not much is known beyond the data included in the analysis at hand. Various justifications and interpretations of noninformative priors\n",
    "have been proposed over the years, including invariance, maximum entropy, and\n",
    "agreement with classical estimators. In our work, we consider noninformative prior\n",
    "distributions to be “reference models” to be used as a standard of comparison or\n",
    "starting point in place of the proper, informative prior distributions that would be\n",
    "appropriate for a full Bayesian analysis.\n",
    "\n",
    "\n",
    "We view any noninformative prior distribution as inherently provisional—after\n",
    "the model has been fit, one should look at the posterior distribution and see if it\n",
    "makes sense. If the posterior distribution does not make sense, this implies that\n",
    "additional prior knowledge is available that has not been included in the model,\n",
    "and that contradicts the assumptions of the prior distribution (or some other part\n",
    "of the model) that has been used. It is then appropriate to go back and alter the\n",
    "model to be more consistent with this external knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: In case of ignorance, or of lack of prior information, one\n",
    "may want to use a prior that is as little informative as\n",
    "possible.\n",
    "\n",
    "An improper prior on $Θ$ is a measurable, nonnegative function\n",
    "$π(·)$ defined on $Θ$ that is not integrable.\n",
    "- In general, one can still define a posterior distribution using an\n",
    "improper prior, using Bayes’ formula. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Jeffreys prior ](https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-slides/MIT18_650F16_Bayesian_Statistics.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi_J(θ) ∝ \\sqrt{\\det I(θ)}$$\n",
    "where $I(θ)$ is the Fisher information matrix of the statistical\n",
    "model associated with $X_1, . . . , X_n$ in the frequentist approach\n",
    "(provided it exists). \n",
    "\n",
    "Jeffreys prior satisfies a reparametrization invariance principle:\n",
    "If $η$ is a reparametrization of $θ$ (i.e.,$ η = φ(θ)$ for some\n",
    "one-to-one map $φ$), then the pdf $\\tilde{π}(·)$ of $η$ satisfies:\n",
    "$$\\tilde{π}(η) ∝ \\sqrt{\\det \\tilde{I}(η)}$$\n",
    "where $\\tilde{I}(η)$ is the Fisher information of the statistical model\n",
    "parametrized by $η$ instead of $θ$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Inverse-Wishart prior](https://dahtah.wordpress.com/2012/03/07/why-an-inverse-wishart-prior-may-not-be-such-a-good-idea/)\n",
    "\n",
    "In statistics, the Wishart distribution is a generalization to multiple dimensions of the gamma distribution. \n",
    "It is a family of probability distributions defined over symmetric, nonnegative-definite matrix-valued random variables (“random matrices”). These distributions are of great importance in the estimation of covariance matrices in multivariate statistics. In Bayesian statistics, the Wishart distribution is the conjugate prior of the inverse covariance-matrix of a multivariate-normal random-vector.\n",
    "In Bayesian statistics, in the context of the multivariate normal distribution, the Wishart distribution is the conjugate prior to the precision matrix $Ω = Σ^{−1}$, where $Σ$ is the covariance matrix.\n",
    "\n",
    "\n",
    "\n",
    "Suppose $X$ is an $p × n$ matrix, each column of which is independently drawn from a $p$-variate normal distribution with zero mean:\n",
    "\n",
    "$$ X_{(i)}{=}(x_{i}^{1},\\dots ,x_{i}^{p})^{T}\\sim N_{p}(0,V)$$\n",
    "\n",
    "Then the Wishart distribution is the probability distribution of the $p × p$ random matrix\n",
    "\n",
    "$$S=\\sum _{i=1}^{n}X_{i}X_{i}^{T}$$\n",
    "\n",
    "known as the scatter matrix. One indicates that S has that probability distribution by writing \n",
    "$S\\sim W_{p}(V,n).$\n",
    "\n",
    "The positive integer $n$ is the number of degrees of freedom. Sometimes this is written $W(V, p, n)$. For $n ≥ p$ the matrix $S$ is invertible with probability 1 if $V$ is invertible.\n",
    "\n",
    "If $p = V = 1$ then this distribution is a chi-squared distribution with $n$ degrees of freedom.\n",
    "\n",
    "\n",
    "\n",
    "The Wishart distribution can be characterized by its probability density function as follows:\n",
    "\n",
    "$$  f_{\\mathbf {X} }(\\mathbf {x} )={\\frac {1}{2^{np/2}\\left|{\\mathbf {V} }\\right|^{n/2}\\Gamma _{p}\\left({\\frac {n}{2}}\\right)}}{\\left|\\mathbf {x} \\right|}^{(n-p-1)/2}e^{-(1/2)\\operatorname {tr} ({\\mathbf {V} }^{-1}\\mathbf {x} )}$$ \n",
    "\n",
    "where $ \\left|{\\mathbf {x} }\\right| $  is the determinant of $ \\mathbf {x} $ and $Γ_p$ is the multivariate gamma function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Why an inverse-Wishart prior may not be such a good idea\n",
    "\n",
    "Inverse Wishart-priors are popular priors over covariance functions. People like them priors because they are conjugate to a Gaussian likelihood, i.e, if you have data \n",
    "$ \\mathbf{y}_{i}\\sim\\mathcal{N}\\left(0,\\mathbf{S}\\right) $\n",
    "so that the $ {\\mathbf{y}_{i}}$‘s are correlated Gaussian vectors, and you wish to infer the correlation matrix $S$, then putting an inverse-Wishart prior on $S$ is convenient because the posterior distribution is very easy to sample from and its mean can be computed analytically.\n",
    "\n",
    "The inverse-Wishart works like this: a Wishart distribution ${\\mathcal{W}\\left(m,\\Lambda\\right)}$ produces random positive definite matrices by first producing ${m}$ Gaussian vectors:\n",
    "$$ \\mathbf{x}_{i}\\sim\\mathcal{N}\\left(0,\\Lambda\\right)$$\n",
    "the Wishart sample is ${m}$-times the sample covariance matrix:\n",
    "\n",
    "$$ \\mathbf{W}=\\sum_{i=1}^{m}\\mathbf{x}_{i}\\mathbf{x}_{i}^{t} $$ \n",
    "\n",
    "which is why for large ${m}$ Wishart samples will look like ${m\\Lambda}$. ${m}$ is the number of degrees of freedom, and we want to keep it low if the prior is to be relatively noninformative.\n",
    "\n",
    "A matrix S has inverse Wishart distribution ${\\mathcal{IW}\\left(m,\\mathbf{M}\\right)}$ if its inverse has Wishart distribution ${\\mathcal{W}\\left(m,\\mathbf{M}^{-1}\\right)}$. \n",
    "\n",
    "Let’s say we want to do Bayesian inference for the correlation of two Gaussian variables. \n",
    "We are interested in one aspect of ${p(\\mathbf{S}|\\mathbf{Y})}$, namely the marginal for the correlation coefficient, i.e. ${r=s_{21}/\\sqrt{s_{11}s_{22}}}$.\n",
    "\n",
    "\n",
    "A good default choice might then be to take ${\\mathbf{S}\\sim\\mathcal{IW}\\left(3,\\mathbf{I}\\right)}$, with $I$ the identity matrix. Under the prior ${E\\left(\\mathbf{S}\\right)=\\mathbf{I}}$.\n",
    "There’s a clear lack of independence, which is even easier to see in the conditional distribution of the correlation coefficient. \n",
    "__Here is a strong dependence between variance and correlation: the prior says that high variance implies high correlation, and low variance implies low-to-moderate correlation. This is a disaster for inference, because it means that correlation will tend to be exagerated if variance is higher than expected, which is the opposite of the shrinkage behaviour we’d like to see.__\n",
    "\n",
    "__There are better priors for covariance matrices out there, but sometimes you might be stuck with the Wishart for computational reasons (for example, it’s the only choice you have in INLA for random effects).__ An option is to estimate the variances first, then tweak the inverse-Wishart prior to have the right scale. Increasing the value of ${m}$ will provide correlation shrinkage. From a Bayesian point of view this is moderately dirty, but preferable to just sticking with the default choice (and see here for a prior choice with good frequentist properties). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
