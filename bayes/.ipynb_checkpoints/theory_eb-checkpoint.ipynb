{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Empirical Bayes method](https://en.wikipedia.org/wiki/Empirical_Bayes_method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical Bayes uses the data to set the hyperparameters of the prior. Performing Bayesian inference with this prior then gets you a sort of shrinkage and can be viewed as an approximation to a hierarchical Bayesian model.\n",
    "\n",
    "\n",
    "EB ignores the uncertainty in the hyper-parameters, whereas HBM attempts to include it in the analysis. HMB is a good idea where there is little data and hence significant uncertainty in the hyper-parameters, which must be accounted for. On the other hand for large datasets EB becomes more attractive as it is generally less computationally expensive and the the volume of data often means the results are much less sensitive to the hyper-parameter settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical Bayes methods are procedures for statistical inference in which the prior distribution is estimated from the data. This approach stands in contrast to standard Bayesian methods, for which the prior distribution is fixed before any data are observed. Despite this difference in perspective, empirical Bayes may be viewed as an approximation to a fully Bayesian treatment of a hierarchical model wherein the parameters at the highest level of the hierarchy are set to their most likely values, instead of being integrated out. Empirical Bayes, also known as maximum marginal likelihood, represents one approach for setting hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [James-Stein estimators](http://www.stats.ox.ac.uk/~reinert/stattheory/chapter1107.pdf)\n",
    "\n",
    "\n",
    "Assume that $θ_i ∼ N (0, τ^2)$, then $p(x|τ^2) = N (0,(1 + τ^2)I_p)$, and\n",
    "the posterior for $θ$ given the data is\n",
    "$$θ|x ∼ N (τ^2/(1 + τ^2) x, 1/(1 + τ)^2I_p)$$\n",
    "\n",
    "Under quadratic loss, the Bayes estimator $δ(x)$ of $θ$ is the posterior\n",
    "mean $τ^2/(1 + τ^2) x$\n",
    "\n",
    "In the empirical Bayes approach, we would use the m.l.e. for \n",
    "$τ^2$\n",
    "and the empirical Bayes estimator is the estimated posterior mean,\n",
    "$$δ^{EB}(x) = \\hat{τ}^2/(1 + \\hat{τ}^2)x = (1 − p/||x||^2)^+ x$$\n",
    "is the truncated James-Stein estimator. It can can be shown to\n",
    "outperform the estimator $δ(x) = x$.\n",
    "\n",
    "Alternatively, the best unbiased estimator of $1/(1 + τ^2)$ is $(p−2)/||x||$\n",
    "giving\n",
    "$$δ^{EB}(x) = (1 − p/|| x ||^2) x$$\n",
    "This is the James-Stein estimator. It can be shown that under\n",
    "quadratic loss function the James-Stein estimator has a risk function\n",
    "that is uniformly better than $δ(x) = x$.\n",
    "\n",
    ">Note: both estimators tend to ”shrink” towards 0. It is now known\n",
    "to be a very general phenomenon that when comparing three or more\n",
    "populations, the sample mean is not the best estimator. ”Shrinkage”\n",
    "estimators are an active area of research\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [James Stein Estimator and its EB Justification. ](https://www2.isye.gatech.edu/~brani/isyebayes/bank/handout8.pdf)\n",
    "\n",
    "Consider the estimation of $θ$ in a model $X ∼ MVN_p(θ, I)$\n",
    "under squared error loss $L(θ, a) = \\sum_i (θ_i − a_i)^2$.\n",
    "\n",
    "For $p = 1$ and $2$, estimator $\\hat{θ} = X$ is admissible (as unique minimax), i.e., no estimator has uniformly\n",
    "better risk. However, for $p ≥ 3$ $X$ is neither unique minimax nor admissible. A better estimator is\n",
    "$$δ_{JS}(X) = (1 − (p − 2)/(\\sum^p_{i=1} X^2_i) ) X$$\n",
    "known as James-Stein estimator.\n",
    "\n",
    "\n",
    "The empirical Bayes justification for δJS(X) is provided next.\n",
    "\n",
    "Suppose that $θ$ has a prior distribution\n",
    "$θ ∼ MVN (0, τ^2\n",
    "I)$,\n",
    "where hyperparameter $τ^2$\n",
    "is not known and will be estimated from the sample, $X$ in this case.\n",
    "\n",
    "The Bayes rule, under squared error loss is\n",
    "$$δ_B(X) = τ^2/(1 + τ^2)X =(1- 1/(1 + τ^2)X$$\n",
    "\n",
    "The method-of-moments estimator of $1/(1+τ^2)$\n",
    " is $(p-2)/(\\sum^p_{i=1} X^2_i) X$\n",
    ", which yields an empirical Bayes estimator\n",
    "$$δ_{EB}(X) = (1 -(p − 2)/(\\sum^p_{i=1} X^2_i)) X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ Hierarchical vs Empirical Bayesian methods](http://www.stats.ox.ac.uk/~reinert/stattheory/chapter1107.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Bayes: $\n",
    "θ|β ∼ π_1(θ|β)$, where $β ∼ π_2(β)$\n",
    "so that then $π(θ) = \\int π_1(θ|β)π_2(β)dβ$.\n",
    "\n",
    "\n",
    "\n",
    "For simulation in hierarchical models: we simulate first from $β$,\n",
    "then, given $β$, we simulate from $θ$. We hope that the distribution of\n",
    "$β$ is easy to simulate, and also that the conditional distribution of $θ$\n",
    "given $β$ is easy to simulate. This approach is particularly useful for\n",
    "MCMC (Markov chain Monte Carlo) methods, e.g.: see next term.\n",
    "\n",
    "\n",
    "Empirical Bayes:\n",
    "$p(x|λ) = \\int f(x|θ)π(θ|λ)dθ$\n",
    "\n",
    "Rather than specifying $λ$, we estimate $λ$ by $\\hat{λ}$, for example by frequentist methods, based on $p(x|λ)$,  $π(θ|x, \\hat{λ})$ \n",
    "is called a pseudo-posterior\n",
    "\n",
    "\n",
    "The empirical Bayes approach\n",
    "- is neither fully Bayesian nor fully frequentist;\n",
    "- depends on $\\hat{λ}$, different $\\hat{λ}$ will lead to different procedures;\n",
    "- if $\\hat{λ}$ is consistent, then asymptotically will lead to coherent Bayesian\n",
    "analysis.\n",
    "- often outperforms classical estimators in empirical terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Empirical and Hierarchical Bayes](https://www.cs.ubc.ca/~schmidtm/Courses/540-W16/L19.pdf)\n",
    "\n",
    "> Why the beta distribution? It’s a flexible distribution that includes uniform as special case (if $α = 1$ and $β = 1$.)\n",
    "\n",
    "- Likelihood $p(x|θ)$. Probability of seeing data given parameters.\n",
    "- Prior $p(θ|α, β)$.\n",
    "Belief that parameters are correct before we’ve seen data.\n",
    "- Posterior $p(θ|x, α, β)$.\n",
    "Probability that parameters are correct after we’ve seen data.\n",
    "- Posterior predictive $p(\\hat{x}|x, α, β)$.\n",
    "Probability of new data given old, integrating over parameters, \n",
    "tells us which prediction is most likely given data and prior.\n",
    "- Marginal likelihood (also called evidence) $p(x|α, β)$.\n",
    "Probability of seeing data given hyper-parameters.\n",
    "\n",
    "Hyper-parameters $α$ and $β$ are like “pseudo-counts” in our mind before we flip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learning the Prior from Data:\n",
    "\n",
    "_Can we use the data to set the hyper-parameters_\n",
    "\n",
    "- In theory: No!\n",
    "    - It would not be a “prior”. \n",
    "- In practice: Yes!\n",
    "    - Approach 1: use a validation set or cross-validation as before.\n",
    "    - Approach 2: optimize the marginal likelihood,\n",
    "$$ p(y|X, λ) = \\int_w p(y|X, w)p(w|λ)dw$$\n",
    "\n",
    "Also called type II maximum likelihood or evidence maximization or empirical Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Overivew of Bayesian Variable Selection\n",
    "\n",
    "[Empirical Bayes vs. fully Bayes variable selection](http://www-stat.wharton.upenn.edu/~edgeorge/Research_papers/CG%20JSPI%202008.pdf)\n",
    "\n",
    "- If we fix $λ$ and use L1-regularization (Bayesian lasso), posterior is not sparse.\n",
    "Probability that a variable is exactly 0 is zero.\n",
    "L1-regularization only lead to sparsity because the MAP point estimate is sparse.\n",
    "- Type II maximum likelihood leads to sparsity in the posterior because variance\n",
    "goes to zero.\n",
    "    - Weird fact: yields sparse solutions (automatic relevance determination).\n",
    "Can send $λ_j → ∞$, concentrating posterior for $w_j$ at 0.\n",
    "This is L2-regularization, but empirical Bayes naturally encouages sparsity.\n",
    "(Non-convex and theory not well understood, but recent work shows:\n",
    "Never performs worse than L1-regularization, and exists cases where it does better)\n",
    "- We can encourage sparsity in Bayesian models using a spike and slab prior:\n",
    "    - Mixture of Dirac delta function 0 and another prior with non-zero variance. Places non-zero posterior weight at exactly 0. Posterior is still non-sparse, but answers the question “what is the probability that variable is non-zero”?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayesian Model Selection and Averaging\n",
    "\n",
    "Bayesian model selection (“type II MAP”): maximize hyper-parameter posterior,\n",
    "which further takes us away from overfitting (thus allowing more complex models).\n",
    "\n",
    "\n",
    "Bayesian model averaging considers posterior over hyper-parameters.\n",
    "We could also maximize marginal likelihood of $γ$, (“type III ML”)\n",
    "\n",
    "- Posterior predictive lets us directly model what we want given hyper-parameters.\n",
    "- Marginal likelihood is probability seeing data given hyper-parameters.\n",
    "- Empirical Bayes optimizes this to set hyper-parameters:\n",
    "    - Allows tuning a large number of hyper-parameters.\n",
    "    - Bayesian Occam’s razor: naturally encourages sparsity and simplicity.\n",
    "- Hierarchical Bayes goes even more Bayesian with prior on hyper-parameters.\n",
    "    - Leads to Bayesian model selection and Bayesian model averaging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Hierarchical Bayes and Empirical Bayes.](https://www2.isye.gatech.edu/~brani/isyebayes/bank/handout8.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Empirical Bayes. ML II Method\n",
    "\n",
    "Empirical Bayes has several formulations. Original formulation of empirical Bayes assumes that past values\n",
    "of $X_i$ and corresponding parameter $θ_i$ are known to the statistician who then on basis of current observation\n",
    "$X_{n+1}$ tries to make inference on unobserved $θ_{n+1}$. Of course, the parameters $θ_i$ are seldom known. However,\n",
    "it may be assumed that the past (and current) $θ$’s are realizations from the same unknown prior distribution.\n",
    "\n",
    "Empirical Bayes is an approach to inference in which the observations are used to select the prior, usually\n",
    "via the marginal distribution. Once the prior is specified, the inference proceed in a standard Bayesian\n",
    "fashion. The use of data to estimate the prior in addition to subsequent use for the inference in empirical\n",
    "Bayes is criticized by subjectivists who consider the prior information exogenous to observations. The\n",
    "repeated use of data is also loaded with perils since it can underestimate modeling errors. Any data is going\n",
    "to be complacent with a model which used the same data to specify some of its features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ML II__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to mimic the maximum likelihood estimation at the marginal level: Select a prior $\\pi$ that maximizes $m_{\\pi}(x)$,\n",
    "given the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offtop:\n",
    "\n",
    "\n",
    "- This ratio is easily numerically evaluated, see mathematica notebook. (p.3)\n",
    "- Berger (1985), Section 4.6 pages 180–195 contains an excellent account on hierarchical models with detailed proofs.\n",
    "- The result (see MATHEMATICA program jeremy.nb on the web site)\n",
    "- Marginal posterior,  marginal distribution\n",
    "- James Stein Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ Hierarchical and Empirical Bayes Analyses](https://www.stat.unipd.it/sites/default/files/bayesian-mod4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Many statistical applications involve multiple parameters that can be regarded as related\n",
    "or connected in some way by the structure of the problem implying that a joint probability\n",
    "model for these parameters should reflect the dependence among them__. For example in a\n",
    "study of effectiveness of cardiac treatments with the patients in hospital $i$ having survival\n",
    "probability $\\theta_i$ it might be reasonable to expect that estimates of the $\\theta_i$'s which represent a sample of hospitals should be related to each other. This is achieved in a natural way\n",
    "if we use a prior distribution in which the $\\theta_i$'s are viewed as a sample from a common\n",
    "population distribution. \n",
    "\n",
    "A key feature of such applications is that the observed data $y_{ij}$\n",
    "on the $j$th unit in the $i$th group can be used to estimate aspects of the population distribution of the $\\theta_i$'s even though they are not potentially observable. It is natural to model\n",
    "such a problem hierarchically with observable outcomes modelled conditionally on certain\n",
    "parameters which themselves are modelled through probability distribution depending on\n",
    "additional parameters say  known as hyperparameters. Such hierarchical thinking helps\n",
    "in understanding multiparameter problems and also plays an important role in developing computational strategies. \n",
    "\n",
    "__Perhaps even more important in practice is that nonhierarchical models are usually\n",
    "unsuitable for hierarchical data; with few parameters they generally cannot large data\n",
    "sets accurately whereas with many parameters they tend to overfit such data in the sense\n",
    "of producing models that fit the existing data well but lead to inferior predictions for future data.__ In contrast hierarchical models can have enough parameters to fit the data well\n",
    "while using a population distribution to structure some dependence into the parameters\n",
    "thereby avoiding problems of overfitting.\n",
    "\n",
    "\n",
    "\n",
    "Hierarchical modelling is useful in both frequentist and Bayesian analyses. Normal\n",
    "mixed linear models and analysis of overdispersion can be viewed as applications of hierarchical modelling in frequentist analysis. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in Berger an EB scenario is one in which known relationships or\n",
    "structures of the coordinates of a parameter vector say  $\\theta$ allow use of the\n",
    "data to estimate some features of the distribution $p_1(\\theta|\\lambda)$  called a prior distribution. For\n",
    "example one may have reason to believe that the $\\theta_i$ are i.i.d. with joint pdf $p_1(\\theta|\\lambda)$ where\n",
    "$p$ is structurally known except for the hyperparameters. A parametric empirical\n",
    "Bayes \t(PEB) procedure is one where  is estimated from the marginal distribution of the\n",
    "observations\n",
    "\n",
    "\n",
    "Closely related to the EB procedure is the HB procedure which models the prior distribution in stages. In the first stage (conditional on  $\\lambda$) $\\theta_i$ are i.i.d. with a\n",
    "prior $p_1(\\cdot|\\lambda)$.  In the second stage a prior distribution say $p_2(\\lambda)$ often noninformative and\n",
    "improper is assigned to $\\lambda$. This is an example of two-stage prior.\n",
    "\n",
    "\n",
    "It is apparent that both the EB and the HB procedures\n",
    "recognize the uncertainty in the prior information but whereas the HB procedure models the uncertainty in the prior information by assigning a distribution \t(often noninformative\n",
    "or improper) to the hyperparameters, the EB procedure attempts to estimate the unknown hyperparameters typically by some classical method such as method of moments, method\n",
    "of maximum likelihood and use the resulting estimated posteriors of the parameters  for inferential purposes. \n",
    "\n",
    "It turns out that the two methods can quite often lead to comparable results especially in the context of point estimation. \n",
    "However when it comes to the question of measuring\n",
    "the standard errors associated with these estimates needed for interval estimation the HB\n",
    "method has a clear edge over a naive EB method. By a naive EB method we mean that\n",
    "EB method based on estimated posterior distribution which does not account fully or in\n",
    "part the uncertainty involved in estimating the hyperparameters. Whereas there are no\n",
    "clear cut measures of standard errors associated with EB point estimates the same is not\n",
    "true with HB estimates. To be precise if one estimates the parameter of interest by its\n",
    "posterior mean then a very natural estimate of the accuracy associated with this estimate\n",
    "is its posterior variance. Estimates of the standard errors associated with EB point estimates usually need an ingenious approximation whereas the posterior variances though\n",
    "often complicated can be found exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Thus a naive EB procedure ignores estimating $V[E( \\theta\t|y, \\xi)|y]$ which amounts to ignoring the\n",
    "uncertainty involved in estimating the prior mean $\\xi$ when estimating the posterior variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First while HB procedure and the non-naive\n",
    "EB procedure tend to produce similar inference the naive EB estimates are quite different\n",
    "and the estimated posterior variances associated with them are too small. Second in the\n",
    "posterior variance of HB estimates the second term namely the variance of the conditional\n",
    "expectation \tcontributes significantly and its omission as often done in\n",
    "naive EB solution will lead to substantial underestimation of the true measure of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can also consider\n",
    "the situation in which the $\\theta_i$ arise from a regression model\n",
    "$$\\theta_i=X^T_i\\beta + e_i$$\n",
    "\n",
    "It is deemed plausible for $\\theta_i$ to be linearly increasing with time:\n",
    "$$\\theta_i = \\beta_1 + \\beta_2 i + e_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Raw data estimates $y_i$ and the regression estimates can be\n",
    "considered two extremes: the $y_i$ would be the natural estimates if no relationships among the\n",
    "$\\theta_i$ were suspected while the $\\hat{\\theta}_{R_i}$ would be the estimates under the specific lower dimensional\n",
    "regression model. The EB and HB estimates are of course averages of these two extremes; note that the EB estimates are slightly closer to the regression estimate.\n",
    "\n",
    "Similarly the raw\n",
    "data variances  and the variances from the regression estimates can be considered\n",
    "to be two extremes: the additional structure of the regression model yields much smaller\n",
    "variances \tonly valid if the model is actually correct of course. Again the EB and HB\n",
    "variances are an average of these extremes and are reasonably similar. Note that the HB\n",
    "variances tend to be smaller than the EB variances for middle i but larger otherwise; they\n",
    "thus mimic the pattern of the regression variances more closely than do the EB variance.\n",
    "\n",
    "Advantages of HB:\n",
    "\n",
    "- We criticized EB\n",
    "because of a failure to consider hyperparameter estimation error; EB theory does not by\n",
    "itself indicate how to incorporate the hyperparameter estimation error in the analysis. On\n",
    "the other hand HB analysis incorporates such errors automatically and is hence generally\n",
    "the more reasonable of the approaches. Sophisticated EB procedures such as that of Morris\n",
    "are usually developed by trying to approximate the HB answer.\n",
    "\n",
    "- Another advantage of the HB approach is that with only slight additional diffculty one can incorporate actual subjective prior information at the second stage.\n",
    "\n",
    "- A third advantage of the HB approach is that it easily produces a greater wealth of\n",
    "information. For instance the posterior variance-covariance matrix presented after \tare\n",
    "substantial and knowledge of them would be important for a variety of statistical analyses.\n",
    "These covariances are easily calculable in HB analysis but would require work to derive in\n",
    "a sophisticated EB fashion.\n",
    "\n",
    "- From a calculational perspective the comparison is something of a toss-up. Standard\n",
    "EB theory requires solution of likelihood equations while the HB approach requires numerical integration. Solutions of likelihood equations is probably somewhat easier particularly when the needed numerical integration is higher dimensional but\n",
    "numerical issues are never clearcut \t(e.g. one has to worry about uniqueness of solutions to the likelihood equation)\n",
    "\n",
    "- In conclusion it appears that the HB approach is the superior methodology for general\n",
    "application. When p is large of course there will be little difference between the two\n",
    "approaches and whichever is more convenient can then be employed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Empirical Hierarchical Bayes Estimation](https://link.springer.com/chapter/10.1007/978-1-4612-2944-5_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is well known that the James-Stein estimates of mean values of several populations can be derived as empirical Bayes\n",
    "estimates assuming a common prior distribution for all the\n",
    "mean values. But the superiority of such estimates over the\n",
    "usual unbiased estimates diminishes as the variability of the\n",
    "true mean values between populations increases. \n",
    "__In such cases\n",
    "it is suggested that the populations may be split into two or\n",
    "more homogeneous groups and the James-Stein procedure applied to the mean values in each group separately.__ In this paper, we introduce a hierarchical prior distribution by considering the mean values within a group to have a common prior\n",
    "with some hyperparameters which are different from group to\n",
    "group. The hyperparameters in different groups are themselves\n",
    "considered to have a common prior distribution possibly with\n",
    "hyper-hyperparameters. Under some conditions on variabilities\n",
    "between and within groups, it is shown that the empirical Bayes\n",
    "estimates derived from a two stage prior distribution on the\n",
    "mean values are better than those obtained by applying the\n",
    "James-Stein procedure to all the mean values in one step or to\n",
    "the mean values in individual groups separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a situation where we have independent samples drawn from\n",
    "a number of populations which can be grouped into a smaller number of\n",
    "clusters such that the populations within a cluster are more homogeneous\n",
    "than those between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of the James-Stein estimates is lost if the parameters\n",
    "under estimation have a large variation. In such a case it may be profitable\n",
    "to consider some natural classification of the parameters into two or more\n",
    "groups and apply the James-Stein procedure separately for the parameters\n",
    "in each group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A natural choice for the hierarchical\n",
    "prior is as follows:\n",
    "1. $\\mu_{i1}, ... , \\mu_{ik}$; (i.e., the parameters of interest in the i-th cluster) are\n",
    "iid with a common probability density $p(\\cdot|\\lambda_i,\\eta)$, $i = 1, ... , p$ depending\n",
    "on a varying parameter $\\lambda_i$ and common parameter $\\eta$.\n",
    "2. The cluster parameters $\\lambda_1, ... , \\lambda_p$ are iid with a common probability\n",
    "density $p(\\cdot | k)$ depending on a parameters $k$.\n",
    "\n",
    "We consider two cases, one when the parameters are known,\n",
    "and another when some or all parameters are unknown but the unknowns\n",
    "are estimable from given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider two types of risk functions: \n",
    "1.Mean dispersion error (MDE) in estimation\n",
    "$$MDE (\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)(\\hat{\\theta} - \\theta)' |\\theta]$$\n",
    "2. Compound mean square error (CMSE) \n",
    "$$ CMSE (\\hat{\\theta}) = tr MDE (\\hat{\\theta}) = \\sum E[(\\hat{\\theta}_i - \\theta_i)^2 | \\theta ] $$\n",
    "3. Bayes MDE \n",
    "$$BMDE (\\hat{\\theta}) = E [MDE (\\hat{\\theta})] $$\n",
    "where the expectation is taken over a specified prior distribution of $\\theta$\n",
    "$$ BCMSE (\\hat{\\theta}) = E[ CMSE (\\hat{\\theta})]$$\n",
    "with respect to a specified prior for $\\theta$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMPIRICAL HIERARCHICAL BAYES METHOD\n",
    "\n",
    "Let us suppose that the parameters under estimation can be grouped in\n",
    "a natural way into clusters and denote the parameters in the i- th cluster.\n",
    "\n",
    "Hierarchical prior is a natural one in situations we consider:\n",
    "1. $\\mu_{i1}, ... , \\mu_{ik}$ are iid random variables having a common prior distribution\n",
    "$$N_1(\\lambda_i,\\sigma^2_1), i=l, ... ,p$$\n",
    "2. $\\lambda_1, ... , \\lambda_p$ are iid random variables having a common prior distribution\n",
    "$$N_1(k,\\sigma^2_2)$$\n",
    "\n",
    "Using the observations  and    the model   for\n",
    "the prior distribution of the parameters, we estimate the parameters $\\mu_{ij}$ by\n",
    "\n",
    "1. applying the J-S procedure separately on the parameters in each cluster,\n",
    "2. considering the parameters in all clusters together and applying the\n",
    "J-S procedure in a single step, and\n",
    "3. the method described based on the hierarchical priors\n",
    "\n",
    "and compare their relative efficiencies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
